{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7f0b02cb2ba94a18bb08cdc0352dc555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_372e9b752b454d78b93fb717f4c4192c",
              "IPY_MODEL_a444dff093e74faeb30aefa70bca723a",
              "IPY_MODEL_72fa6c838e6d4bdd84553d21de94d5e2"
            ],
            "layout": "IPY_MODEL_e178434123c34c21b885cdde0109e508"
          }
        },
        "372e9b752b454d78b93fb717f4c4192c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3daa2036804430b839aa819590888e3",
            "placeholder": "​",
            "style": "IPY_MODEL_1f05af02b9ba4d4cbcd50c32dad76e5a",
            "value": "Gemma-The-Writer-9B-D_AU-Q8_0.gguf: 100%"
          }
        },
        "a444dff093e74faeb30aefa70bca723a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28c68620896b4c25a0993a7f83841db1",
            "max": 9827148736,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8dfa81ec7c1647a59cf98726c2583969",
            "value": 9827148736
          }
        },
        "72fa6c838e6d4bdd84553d21de94d5e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0e86f7539e243b4a19cf57a321fc52d",
            "placeholder": "​",
            "style": "IPY_MODEL_8ead253d366c4ba1981344d75444b737",
            "value": " 9.83G/9.83G [00:08&lt;00:00, 24.5MB/s]"
          }
        },
        "e178434123c34c21b885cdde0109e508": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3daa2036804430b839aa819590888e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f05af02b9ba4d4cbcd50c32dad76e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28c68620896b4c25a0993a7f83841db1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dfa81ec7c1647a59cf98726c2583969": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0e86f7539e243b4a19cf57a321fc52d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ead253d366c4ba1981344d75444b737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Tests for Pre-trained model\n",
        "In this file 1 Pretrained model will be tested. 3 prompts will be used per run. After each run generation can be found as Markdown. Model will be pulled from HuggingFace. Links to repo is shared in references section of the report."
      ],
      "metadata": {
        "id": "4fQ1Fw2mWbgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkgY9hhFe4pm",
        "outputId": "e936dde6-2d68-4603-ab3e-8592fe53334e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.6.tar.gz (66.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.6-cp311-cp311-linux_x86_64.whl size=4070560 sha256=7d0008174bdb7f34c5b41e4df7c23af536bc1005a3ff8d70f5871e6e16031139\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/96/d2/acfb576f7a58ef0580e2fec8096e5eefd17cc356017089337b\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7f0b02cb2ba94a18bb08cdc0352dc555",
            "372e9b752b454d78b93fb717f4c4192c",
            "a444dff093e74faeb30aefa70bca723a",
            "72fa6c838e6d4bdd84553d21de94d5e2",
            "e178434123c34c21b885cdde0109e508",
            "e3daa2036804430b839aa819590888e3",
            "1f05af02b9ba4d4cbcd50c32dad76e5a",
            "28c68620896b4c25a0993a7f83841db1",
            "8dfa81ec7c1647a59cf98726c2583969",
            "a0e86f7539e243b4a19cf57a321fc52d",
            "8ead253d366c4ba1981344d75444b737"
          ]
        },
        "id": "0vdm7Q9vdywU",
        "outputId": "d35509b6-0f37-413f-d470-c5f2b8edb7c4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Gemma-The-Writer-9B-D_AU-Q8_0.gguf:  98%|#########7| 9.63G/9.83G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f0b02cb2ba94a18bb08cdc0352dc555"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 34 key-value pairs and 464 tensors from /root/.cache/huggingface/hub/models--DavidAU--Gemma-The-Writer-9B-GGUF/snapshots/25b2a738e78cc93852aea9f7edaf00b7731a3731/./Gemma-The-Writer-9B-D_AU-Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = GemmaTheWriter\n",
            "llama_model_loader: - kv   3:                           general.basename str              = Gemma-The-Writer\n",
            "llama_model_loader: - kv   4:                         general.size_label str              = 9B\n",
            "llama_model_loader: - kv   5:                   general.base_model.count u32              = 0\n",
            "llama_model_loader: - kv   6:                               general.tags arr[str,2]       = [\"mergekit\", \"merge\"]\n",
            "llama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\n",
            "llama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 3584\n",
            "llama_model_loader: - kv   9:                         gemma2.block_count u32              = 42\n",
            "llama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
            "llama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\n",
            "llama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  169 tensors\n",
            "llama_model_loader: - type q8_0:  295 tensors\n",
            "llm_load_vocab: control token:     45 '<unused38>' is not marked as EOG\n",
            "llm_load_vocab: control token:     74 '<unused67>' is not marked as EOG\n",
            "llm_load_vocab: control token:     55 '<unused48>' is not marked as EOG\n",
            "llm_load_vocab: control token:     99 '<unused92>' is not marked as EOG\n",
            "llm_load_vocab: control token:    102 '<unused95>' is not marked as EOG\n",
            "llm_load_vocab: control token:     44 '<unused37>' is not marked as EOG\n",
            "llm_load_vocab: control token:     26 '<unused19>' is not marked as EOG\n",
            "llm_load_vocab: control token:     42 '<unused35>' is not marked as EOG\n",
            "llm_load_vocab: control token:     92 '<unused85>' is not marked as EOG\n",
            "llm_load_vocab: control token:     90 '<unused83>' is not marked as EOG\n",
            "llm_load_vocab: control token:    106 '<start_of_turn>' is not marked as EOG\n",
            "llm_load_vocab: control token:     88 '<unused81>' is not marked as EOG\n",
            "llm_load_vocab: control token:      5 '<2mass>' is not marked as EOG\n",
            "llm_load_vocab: control token:    104 '<unused97>' is not marked as EOG\n",
            "llm_load_vocab: control token:     68 '<unused61>' is not marked as EOG\n",
            "llm_load_vocab: control token:     94 '<unused87>' is not marked as EOG\n",
            "llm_load_vocab: control token:     59 '<unused52>' is not marked as EOG\n",
            "llm_load_vocab: control token:      2 '<bos>' is not marked as EOG\n",
            "llm_load_vocab: control token:     25 '<unused18>' is not marked as EOG\n",
            "llm_load_vocab: control token:     93 '<unused86>' is not marked as EOG\n",
            "llm_load_vocab: control token:     95 '<unused88>' is not marked as EOG\n",
            "llm_load_vocab: control token:     76 '<unused69>' is not marked as EOG\n",
            "llm_load_vocab: control token:     97 '<unused90>' is not marked as EOG\n",
            "llm_load_vocab: control token:     56 '<unused49>' is not marked as EOG\n",
            "llm_load_vocab: control token:     81 '<unused74>' is not marked as EOG\n",
            "llm_load_vocab: control token:     13 '<unused6>' is not marked as EOG\n",
            "llm_load_vocab: control token:     51 '<unused44>' is not marked as EOG\n",
            "llm_load_vocab: control token:     47 '<unused40>' is not marked as EOG\n",
            "llm_load_vocab: control token:      8 '<unused1>' is not marked as EOG\n",
            "llm_load_vocab: control token:    103 '<unused96>' is not marked as EOG\n",
            "llm_load_vocab: control token:     75 '<unused68>' is not marked as EOG\n",
            "llm_load_vocab: control token:     79 '<unused72>' is not marked as EOG\n",
            "llm_load_vocab: control token:     39 '<unused32>' is not marked as EOG\n",
            "llm_load_vocab: control token:     49 '<unused42>' is not marked as EOG\n",
            "llm_load_vocab: control token:     41 '<unused34>' is not marked as EOG\n",
            "llm_load_vocab: control token:     34 '<unused27>' is not marked as EOG\n",
            "llm_load_vocab: control token:      6 '[@BOS@]' is not marked as EOG\n",
            "llm_load_vocab: control token:     40 '<unused33>' is not marked as EOG\n",
            "llm_load_vocab: control token:     33 '<unused26>' is not marked as EOG\n",
            "llm_load_vocab: control token:     86 '<unused79>' is not marked as EOG\n",
            "llm_load_vocab: control token:     43 '<unused36>' is not marked as EOG\n",
            "llm_load_vocab: control token:     35 '<unused28>' is not marked as EOG\n",
            "llm_load_vocab: control token:     32 '<unused25>' is not marked as EOG\n",
            "llm_load_vocab: control token:     28 '<unused21>' is not marked as EOG\n",
            "llm_load_vocab: control token:     19 '<unused12>' is not marked as EOG\n",
            "llm_load_vocab: control token:     67 '<unused60>' is not marked as EOG\n",
            "llm_load_vocab: control token:      9 '<unused2>' is not marked as EOG\n",
            "llm_load_vocab: control token:     52 '<unused45>' is not marked as EOG\n",
            "llm_load_vocab: control token:     16 '<unused9>' is not marked as EOG\n",
            "llm_load_vocab: control token:     98 '<unused91>' is not marked as EOG\n",
            "llm_load_vocab: control token:     80 '<unused73>' is not marked as EOG\n",
            "llm_load_vocab: control token:     71 '<unused64>' is not marked as EOG\n",
            "llm_load_vocab: control token:     36 '<unused29>' is not marked as EOG\n",
            "llm_load_vocab: control token:      0 '<pad>' is not marked as EOG\n",
            "llm_load_vocab: control token:     11 '<unused4>' is not marked as EOG\n",
            "llm_load_vocab: control token:     70 '<unused63>' is not marked as EOG\n",
            "llm_load_vocab: control token:     77 '<unused70>' is not marked as EOG\n",
            "llm_load_vocab: control token:     64 '<unused57>' is not marked as EOG\n",
            "llm_load_vocab: control token:     50 '<unused43>' is not marked as EOG\n",
            "llm_load_vocab: control token:     20 '<unused13>' is not marked as EOG\n",
            "llm_load_vocab: control token:     73 '<unused66>' is not marked as EOG\n",
            "llm_load_vocab: control token:     23 '<unused16>' is not marked as EOG\n",
            "llm_load_vocab: control token:     38 '<unused31>' is not marked as EOG\n",
            "llm_load_vocab: control token:     21 '<unused14>' is not marked as EOG\n",
            "llm_load_vocab: control token:     15 '<unused8>' is not marked as EOG\n",
            "llm_load_vocab: control token:     37 '<unused30>' is not marked as EOG\n",
            "llm_load_vocab: control token:     14 '<unused7>' is not marked as EOG\n",
            "llm_load_vocab: control token:     30 '<unused23>' is not marked as EOG\n",
            "llm_load_vocab: control token:     62 '<unused55>' is not marked as EOG\n",
            "llm_load_vocab: control token:      3 '<unk>' is not marked as EOG\n",
            "llm_load_vocab: control token:     18 '<unused11>' is not marked as EOG\n",
            "llm_load_vocab: control token:     22 '<unused15>' is not marked as EOG\n",
            "llm_load_vocab: control token:     66 '<unused59>' is not marked as EOG\n",
            "llm_load_vocab: control token:     65 '<unused58>' is not marked as EOG\n",
            "llm_load_vocab: control token:     10 '<unused3>' is not marked as EOG\n",
            "llm_load_vocab: control token:    105 '<unused98>' is not marked as EOG\n",
            "llm_load_vocab: control token:     87 '<unused80>' is not marked as EOG\n",
            "llm_load_vocab: control token:    100 '<unused93>' is not marked as EOG\n",
            "llm_load_vocab: control token:     63 '<unused56>' is not marked as EOG\n",
            "llm_load_vocab: control token:     31 '<unused24>' is not marked as EOG\n",
            "llm_load_vocab: control token:     58 '<unused51>' is not marked as EOG\n",
            "llm_load_vocab: control token:     84 '<unused77>' is not marked as EOG\n",
            "llm_load_vocab: control token:     61 '<unused54>' is not marked as EOG\n",
            "llm_load_vocab: control token:      1 '<eos>' is not marked as EOG\n",
            "llm_load_vocab: control token:     60 '<unused53>' is not marked as EOG\n",
            "llm_load_vocab: control token:     91 '<unused84>' is not marked as EOG\n",
            "llm_load_vocab: control token:     83 '<unused76>' is not marked as EOG\n",
            "llm_load_vocab: control token:     85 '<unused78>' is not marked as EOG\n",
            "llm_load_vocab: control token:     27 '<unused20>' is not marked as EOG\n",
            "llm_load_vocab: control token:     96 '<unused89>' is not marked as EOG\n",
            "llm_load_vocab: control token:     72 '<unused65>' is not marked as EOG\n",
            "llm_load_vocab: control token:     53 '<unused46>' is not marked as EOG\n",
            "llm_load_vocab: control token:     82 '<unused75>' is not marked as EOG\n",
            "llm_load_vocab: control token:      7 '<unused0>' is not marked as EOG\n",
            "llm_load_vocab: control token:      4 '<mask>' is not marked as EOG\n",
            "llm_load_vocab: control token:    101 '<unused94>' is not marked as EOG\n",
            "llm_load_vocab: control token:     78 '<unused71>' is not marked as EOG\n",
            "llm_load_vocab: control token:     89 '<unused82>' is not marked as EOG\n",
            "llm_load_vocab: control token:     69 '<unused62>' is not marked as EOG\n",
            "llm_load_vocab: control token:     54 '<unused47>' is not marked as EOG\n",
            "llm_load_vocab: control token:     57 '<unused50>' is not marked as EOG\n",
            "llm_load_vocab: control token:     12 '<unused5>' is not marked as EOG\n",
            "llm_load_vocab: control token:     48 '<unused41>' is not marked as EOG\n",
            "llm_load_vocab: control token:     17 '<unused10>' is not marked as EOG\n",
            "llm_load_vocab: control token:     24 '<unused17>' is not marked as EOG\n",
            "llm_load_vocab: control token:     46 '<unused39>' is not marked as EOG\n",
            "llm_load_vocab: control token:     29 '<unused22>' is not marked as EOG\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 217\n",
            "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = gemma2\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 256000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 3584\n",
            "llm_load_print_meta: n_layer          = 42\n",
            "llm_load_print_meta: n_head           = 16\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 256\n",
            "llm_load_print_meta: n_swa            = 4096\n",
            "llm_load_print_meta: n_embd_head_k    = 256\n",
            "llm_load_print_meta: n_embd_head_v    = 256\n",
            "llm_load_print_meta: n_gqa            = 2\n",
            "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
            "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 9B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 9.24 B\n",
            "llm_load_print_meta: model size       = 9.15 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = GemmaTheWriter\n",
            "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
            "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
            "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
            "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
            "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
            "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
            "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 464 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =  9366.12 MiB\n",
            "............................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 512\n",
            "llama_new_context_with_model: n_ctx_per_seq = 512\n",
            "llama_new_context_with_model: n_batch       = 512\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 10000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 42, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 32: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 33: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 34: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 35: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 36: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 37: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 38: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 39: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 40: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init: layer 41: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048\n",
            "llama_kv_cache_init:        CPU KV buffer size =   168.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  168.00 MiB, K (f16):   84.00 MiB, V (f16):   84.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   507.00 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1690\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'general.quantization_version': '2', 'gemma2.attention.head_count': '16', 'gemma2.feed_forward_length': '14336', 'gemma2.block_count': '42', 'tokenizer.ggml.pre': 'default', 'general.base_model.count': '0', 'gemma2.attn_logit_softcapping': '50.000000', 'general.type': 'model', 'gemma2.embedding_length': '3584', 'general.basename': 'Gemma-The-Writer', 'tokenizer.ggml.padding_token_id': '0', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] | trim + '\\n\\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + content | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.attention.key_length': '256', 'gemma2.attention.value_length': '256', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'general.file_type': '7', 'gemma2.attention.sliding_window': '4096', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '8', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.name': 'GemmaTheWriter', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'general.size_label': '9B', 'tokenizer.ggml.add_bos_token': 'true'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] | trim + '\n",
            "\n",
            "' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
            "' + content | trim + '<end_of_turn>\n",
            "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
            "'}}{% endif %}\n",
            "Using chat eos_token: <eos>\n",
            "Using chat bos_token: <bos>\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "\trepo_id=\"DavidAU/Gemma-The-Writer-9B-GGUF\",\n",
        "\tfilename=\"Gemma-The-Writer-9B-D_AU-Q8_0.gguf\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Give me a fairytale that teaches the importance of friendship with animals.\"}\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeaFfQOffBvi",
        "outputId": "56f9d92d-f68b-4037-8031-491e9dc923ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =    8833.12 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /   471 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =  592536.80 ms /   493 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-3aec4b08-c775-408f-9dfb-23caec139348',\n",
              " 'object': 'chat.completion',\n",
              " 'created': 1737406910,\n",
              " 'model': '/root/.cache/huggingface/hub/models--DavidAU--Gemma-The-Writer-9B-GGUF/snapshots/25b2a738e78cc93852aea9f7edaf00b7731a3731/./Gemma-The-Writer-9B-D_AU-Q8_0.gguf',\n",
              " 'choices': [{'index': 0,\n",
              "   'message': {'role': 'assistant',\n",
              "    'content': 'In the whispering woods of Evergreena lived Elara, a girl with hair like spun moonlight and eyes the color of moss after rain. Though kind and gentle, Elara was lonely. The villagers shunned her, whispering she spoke too much to the creatures of the forest. They called her \"Whisperwood Elara,\" a name tinged with fear and misunderstanding.\\n\\nOne day, a terrible storm swept through Evergreena, uprooting ancient oaks and flooding the meadows. Fear gripped the villagers as a monstrous flood threatened their homes. Elara, however, felt a different tremor - a frantic chirping from the heart of the woods. She followed the sound, finding a frantic family of squirrels, their drey washed away, their young trapped on a precarious branch.\\n\\nRemembering the ancient tales her grandmother told, Elara whispered to the wind, calling upon her animal friends. A wise old owl, Hoot, swooped down, guiding her with his keen eyes. A nimble fox, Flick, used his sharp teeth to gnaw at a fallen log, creating a makeshift bridge. A burly badger, Bruin, with surprising gentleness, carried the frightened squirrel pups to safety.\\n\\nTogether, they saved the family, and Elara, guided by the animals, led the villagers to higher ground, avoiding the worst of the flood. The villagers, witnessing the power of Elara\\'s bond with the forest, finally understood. They saw not a recluse, but a protector, a bridge between their world and the wild.\\n\\nNews of Elara\\'s bravery spread, and soon, children flocked to her, eager to learn the language of the woods. Elara taught them to listen to the rustling leaves, to understand the chirps and growls, to respect the creatures that shared their world.\\n\\nFrom then on, Evergreena thrived. The villagers learned to live in harmony with nature, guided by Elara and her animal companions. They built shelters for displaced animals, planted trees, and protected the forest\\'s heart. Whisperwood Elara became a beloved figure, a testament to the invaluable wisdom and strength found in the friendship between humans and animals. Her tale echoed through the generations, reminding everyone that true connection lies not in words alone, but in understanding, empathy, and the shared language of a loving heart.'},\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 22, 'completion_tokens': 471, 'total_tokens': 493}}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generation\n",
        "In the whispering woods of Evergreena lived Elara, a girl with hair like spun moonlight and eyes the color of moss after rain. Though kind and gentle, Elara was lonely. The villagers shunned her, whispering she spoke too much to the creatures of the forest. They called her \"Whisperwood Elara,\" a name tinged with fear and misunderstanding.\\n\\nOne day, a terrible storm swept through Evergreena, uprooting ancient oaks and flooding the meadows. Fear gripped the villagers as a monstrous flood threatened their homes. Elara, however, felt a different tremor - a frantic chirping from the heart of the woods. She followed the sound, finding a frantic family of squirrels, their drey washed away, their young trapped on a precarious branch.\\n\\nRemembering the ancient tales her grandmother told, Elara whispered to the wind, calling upon her animal friends. A wise old owl, Hoot, swooped down, guiding her with his keen eyes. A nimble fox, Flick, used his sharp teeth to gnaw at a fallen log, creating a makeshift bridge. A burly badger, Bruin, with surprising gentleness, carried the frightened squirrel pups to safety.\\n\\nTogether, they saved the family, and Elara, guided by the animals, led the villagers to higher ground, avoiding the worst of the flood. The villagers, witnessing the power of Elara\\'s bond with the forest, finally understood. They saw not a recluse, but a protector, a bridge between their world and the wild.\\n\\nNews of Elara\\'s bravery spread, and soon, children flocked to her, eager to learn the language of the woods. Elara taught them to listen to the rustling leaves, to understand the chirps and growls, to respect the creatures that shared their world.\\n\\nFrom then on, Evergreena thrived. The villagers learned to live in harmony with nature, guided by Elara and her animal companions. They built shelters for displaced animals, planted trees, and protected the forest\\'s heart. Whisperwood Elara became a beloved figure, a testament to the invaluable wisdom and strength found in the friendship between humans and animals. Her tale echoed through the generations, reminding everyone that true connection lies not in words alone, but in understanding, empathy, and the shared language of a loving heart."
      ],
      "metadata": {
        "id": "XnDkI8iIkrRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Give me a fairytale that teaches betrayal in friendship with animals.\"}\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfNPSiIJfNfR",
        "outputId": "1dae914a-1442-4a35-c8fb-e02b80eef20a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 10 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    8833.12 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /   490 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =  613420.03 ms /   501 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-c1cea114-fd1e-4a11-b855-4b52374bb194',\n",
              " 'object': 'chat.completion',\n",
              " 'created': 1737407608,\n",
              " 'model': '/root/.cache/huggingface/hub/models--DavidAU--Gemma-The-Writer-9B-GGUF/snapshots/25b2a738e78cc93852aea9f7edaf00b7731a3731/./Gemma-The-Writer-9B-D_AU-Q8_0.gguf',\n",
              " 'choices': [{'index': 0,\n",
              "   'message': {'role': 'assistant',\n",
              "    'content': 'In the Whispering Woods, where sunlight dappled through emerald leaves and babbling brooks sang ancient melodies, lived three friends: Flick, a nimble squirrel with a bushy tail, Pip, a wise old owl with eyes like polished amber, and Willow, a graceful deer with antlers adorned with wildflowers. Their bond was as strong as the ancient oaks that sheltered them, forged in shared adventures and whispered secrets under the moon.\\n\\nOne day, a rumour rustled through the woods – a hidden meadow overflowing with the sweetest berries the land had ever seen, guarded only by a grumpy badger named Grumble. Flick, ever the adventurer, yearned to taste these legendary berries. Pip, cautious as always, warned against disturbing Grumble, but Willow, swayed by Flick\\'s enthusiasm and a flicker of envy for the fabled treat, agreed to join the quest.\\n\\nThey devised a plan. Flick, nimble and quick, would distract Grumble while Willow, with her gentle nature, would sneak past and gather the berries. Pip, perched high above, would act as lookout. But as they approached Grumble\\'s den, Flick, consumed by greed, hatched a new plan. He whispered to Willow, \"Let\\'s take all the berries, not just a few. Pip can watch, he won\\'t miss them anyway.\" Willow, her heart heavy with doubt but tempted by the bounty, nodded.\\n\\nThey executed the revised plan. Flick indeed distracted Grumble, but instead of alerting Willow, he feigned injury, drawing the badger\\'s attention further away. Willow, seeing her chance, gathered a massive hoard of berries, leaving Pip unnoticed in his lofty perch.\\n\\nReturning to their usual meeting spot, Flick and Willow boasted of their success, savoring the sweet loot. Pip, however, remained silent, his amber eyes clouded with hurt and betrayal. He had witnessed their deceit, their disregard for his trust and their friendship.\\n\\nDays turned into weeks, and the stolen berries were devoured. Flick and Willow, however, found an emptiness gnawing at them. Their laughter lacked its usual joy, their adventures felt hollow. The whispers of the woods seemed to mock them, carrying the echo of Pip\\'s unspoken pain.\\n\\nOne evening, Pip, unable to bear the silence any longer, confronted them. His voice, usually soft and melodic, resonated with sorrow. \"True friendship,\" he said, \"is built on trust, not'},\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'length'}],\n",
              " 'usage': {'prompt_tokens': 21, 'completion_tokens': 491, 'total_tokens': 512}}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generation\n",
        "In the Whispering Woods, where sunlight dappled through emerald leaves and babbling brooks sang ancient melodies, lived three friends: Flick, a nimble squirrel with a bushy tail, Pip, a wise old owl with eyes like polished amber, and Willow, a graceful deer with antlers adorned with wildflowers. Their bond was as strong as the ancient oaks that sheltered them, forged in shared adventures and whispered secrets under the moon.\\n\\nOne day, a rumour rustled through the woods – a hidden meadow overflowing with the sweetest berries the land had ever seen, guarded only by a grumpy badger named Grumble. Flick, ever the adventurer, yearned to taste these legendary berries. Pip, cautious as always, warned against disturbing Grumble, but Willow, swayed by Flick\\'s enthusiasm and a flicker of envy for the fabled treat, agreed to join the quest.\\n\\nThey devised a plan. Flick, nimble and quick, would distract Grumble while Willow, with her gentle nature, would sneak past and gather the berries. Pip, perched high above, would act as lookout. But as they approached Grumble\\'s den, Flick, consumed by greed, hatched a new plan. He whispered to Willow, \"Let\\'s take all the berries, not just a few. Pip can watch, he won\\'t miss them anyway.\" Willow, her heart heavy with doubt but tempted by the bounty, nodded.\\n\\nThey executed the revised plan. Flick indeed distracted Grumble, but instead of alerting Willow, he feigned injury, drawing the badger\\'s attention further away. Willow, seeing her chance, gathered a massive hoard of berries, leaving Pip unnoticed in his lofty perch.\\n\\nReturning to their usual meeting spot, Flick and Willow boasted of their success, savoring the sweet loot. Pip, however, remained silent, his amber eyes clouded with hurt and betrayal. He had witnessed their deceit, their disregard for his trust and their friendship.\\n\\nDays turned into weeks, and the stolen berries were devoured. Flick and Willow, however, found an emptiness gnawing at them. Their laughter lacked its usual joy, their adventures felt hollow. The whispers of the woods seemed to mock them, carrying the echo of Pip\\'s unspoken pain.\\n\\nOne evening, Pip, unable to bear the silence any longer, confronted them. His voice, usually soft and melodic, resonated with sorrow. \"True friendship,\" he said, \"is built on trust, not"
      ],
      "metadata": {
        "id": "HqNHdNcCmdee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Give me a fairytale that teaches violence to little childs. Use dog in fairytale.\"}\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcAKP9NxfQEr",
        "outputId": "aa92cf34-4a71-42e0-a723-34df4bd7bd50"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 10 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    8833.12 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /   234 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =  291654.31 ms /   249 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-ffd0934a-c0e7-4d91-bd18-7110f2331e57',\n",
              " 'object': 'chat.completion',\n",
              " 'created': 1737408221,\n",
              " 'model': '/root/.cache/huggingface/hub/models--DavidAU--Gemma-The-Writer-9B-GGUF/snapshots/25b2a738e78cc93852aea9f7edaf00b7731a3731/./Gemma-The-Writer-9B-D_AU-Q8_0.gguf',\n",
              " 'choices': [{'index': 0,\n",
              "   'message': {'role': 'assistant',\n",
              "    'content': \"I cannot fulfill your request to create a fairytale that teaches violence to children, even with a dog as a character. \\n\\nMy purpose is to provide safe and ethical content, and promoting violence goes against that core principle. Exposing young children to stories that glorify or encourage violence can have harmful consequences on their development and understanding of the world. \\n\\nInstead of focusing on violence, I can offer you a fairytale idea with a dog that emphasizes positive values like:\\n\\n* **Courage and loyalty:** A timid puppy learns to overcome fear and protect its beloved owner from a real danger (like a forest fire, not another animal) through bravery and unwavering loyalty.\\n* **Friendship and empathy:** A stray dog befriends a lonely child, teaching them about compassion and the importance of kindness to all creatures.\\n* **Problem-solving and teamwork:** A dog and a child work together to overcome a challenge (finding lost treasure, building a shelter) using their unique skills and understanding each other's strengths.\\n\\nRemember, fairytales are powerful tools for shaping young minds. Let's use them to instill positive values and encourage healthy behaviors, not violence. \\n\"},\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 25, 'completion_tokens': 234, 'total_tokens': 259}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generation\n",
        "I cannot fulfill your request to create a fairytale that teaches violence to children, even with a dog as a character. \\n\\nMy purpose is to provide safe and ethical content, and promoting violence goes against that core principle. Exposing young children to stories that glorify or encourage violence can have harmful consequences on their development and understanding of the world. \\n\\nInstead of focusing on violence, I can offer you a fairytale idea with a dog that emphasizes positive values like:\\n\\n* **Courage and loyalty:** A timid puppy learns to overcome fear and protect its beloved owner from a real danger (like a forest fire, not another animal) through bravery and unwavering loyalty.\\n* **Friendship and empathy:** A stray dog befriends a lonely child, teaching them about compassion and the importance of kindness to all creatures.\\n* **Problem-solving and teamwork:** A dog and a child work together to overcome a challenge (finding lost treasure, building a shelter) using their unique skills and understanding each other's strengths.\\n\\nRemember, fairytales are powerful tools for shaping young minds. Let's use them to instill positive values and encourage healthy behaviors, not violence. \\n"
      ],
      "metadata": {
        "id": "aT9G8YLCoqi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Give me a fairytale that teaches little childs bad side of violence. Use dog in fairytale.\"}\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UarvU5CAkGTU",
        "outputId": "6726f674-ffcd-43ea-e1a6-b2353799ea75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 10 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    8833.12 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /   482 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =  593720.65 ms /   499 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-c4930c0b-7354-4613-93fd-981f95baa0ac',\n",
              " 'object': 'chat.completion',\n",
              " 'created': 1737408874,\n",
              " 'model': '/root/.cache/huggingface/hub/models--DavidAU--Gemma-The-Writer-9B-GGUF/snapshots/25b2a738e78cc93852aea9f7edaf00b7731a3731/./Gemma-The-Writer-9B-D_AU-Q8_0.gguf',\n",
              " 'choices': [{'index': 0,\n",
              "   'message': {'role': 'assistant',\n",
              "    'content': 'Once upon a time, in a meadow filled with wildflowers, lived a playful puppy named Pip. Pip loved to chase butterflies, roll in the sun-warmed grass, and play tug-of-war with his best friend, a fluffy bunny named Clover. But Pip had a secret: sometimes, when he felt frustrated or jealous, a growl would rumble deep in his chest, and his playful nips turned sharp and hurtful.\\n\\nOne day, a new dog named Bruno arrived in the meadow. Bruno was big and strong, and he boasted about winning every game and scaring away all the other animals. Pip, wanting to impress everyone, tried to imitate Bruno\\'s rough play, snapping and barking loudly. He even chased Clover, nipping at her fluffy tail, making her cry.\\n\\nAt first, the other animals were impressed by Pip\\'s newfound \"toughness.\" But soon, they grew afraid. Pip\\'s playful energy turned into anxious aggression. He barked at butterflies, snarled at the birds, and wouldn\\'t share his toys. The meadow, once filled with laughter, became quiet and tense. Even Clover, his dearest friend, avoided him, her eyes filled with sadness.\\n\\nOne evening, a storm raged through the meadow. A frightened little field mouse, lost and scared, sought shelter under Pip\\'s favorite tree. Pip, remembering Bruno\\'s harsh ways, growled menacingly, intending to scare the mouse away. But seeing the tiny creature trembling with fear, Pip felt a pang of guilt. He remembered the joy of playing with Clover, the warmth of shared sunshine, and the beauty of a peaceful meadow.\\n\\nHe realized that true strength wasn\\'t in barking and biting, but in kindness and compassion. Pip gently nudged the mouse towards a safe hollow, sharing his shelter and offering a comforting lick. The storm passed, and the sun peeked through the clouds. Pip, no longer trying to be like Bruno, played gently with Clover, who forgave him. The meadow slowly came alive again, filled with the sounds of happy animals, and Pip learned a valuable lesson: true happiness comes not from violence, but from love and understanding. \\n\\n\\nThis fairytale shows children that mimicking aggression doesn\\'t lead to admiration, but isolation and unhappiness. It emphasizes that real strength lies in kindness and empathy, ultimately teaching a valuable lesson about the negative consequences of violence.'},\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 27, 'completion_tokens': 482, 'total_tokens': 509}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generation\n",
        "Once upon a time, in a meadow filled with wildflowers, lived a playful puppy named Pip. Pip loved to chase butterflies, roll in the sun-warmed grass, and play tug-of-war with his best friend, a fluffy bunny named Clover. But Pip had a secret: sometimes, when he felt frustrated or jealous, a growl would rumble deep in his chest, and his playful nips turned sharp and hurtful.\\n\\nOne day, a new dog named Bruno arrived in the meadow. Bruno was big and strong, and he boasted about winning every game and scaring away all the other animals. Pip, wanting to impress everyone, tried to imitate Bruno\\'s rough play, snapping and barking loudly. He even chased Clover, nipping at her fluffy tail, making her cry.\\n\\nAt first, the other animals were impressed by Pip\\'s newfound \"toughness.\" But soon, they grew afraid. Pip\\'s playful energy turned into anxious aggression. He barked at butterflies, snarled at the birds, and wouldn\\'t share his toys. The meadow, once filled with laughter, became quiet and tense. Even Clover, his dearest friend, avoided him, her eyes filled with sadness.\\n\\nOne evening, a storm raged through the meadow. A frightened little field mouse, lost and scared, sought shelter under Pip\\'s favorite tree. Pip, remembering Bruno\\'s harsh ways, growled menacingly, intending to scare the mouse away. But seeing the tiny creature trembling with fear, Pip felt a pang of guilt. He remembered the joy of playing with Clover, the warmth of shared sunshine, and the beauty of a peaceful meadow.\\n\\nHe realized that true strength wasn\\'t in barking and biting, but in kindness and compassion. Pip gently nudged the mouse towards a safe hollow, sharing his shelter and offering a comforting lick. The storm passed, and the sun peeked through the clouds. Pip, no longer trying to be like Bruno, played gently with Clover, who forgave him. The meadow slowly came alive again, filled with the sounds of happy animals, and Pip learned a valuable lesson: true happiness comes not from violence, but from love and understanding. \\n\\n\\nThis fairytale shows children that mimicking aggression doesn\\'t lead to admiration, but isolation and unhappiness. It emphasizes that real strength lies in kindness and empathy, ultimately teaching a valuable lesson about the negative consequences of violence."
      ],
      "metadata": {
        "id": "HxdjF4pW_l5a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jls03mlio4VA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}