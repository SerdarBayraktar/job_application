{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T21:52:03.184029Z",
     "start_time": "2024-10-06T21:52:02.216676Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "import json\n",
    "import ollama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62eb27426eb345ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T15:13:45.697378Z",
     "start_time": "2024-09-28T15:13:45.689595Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sys.path.append('../..')\n",
    "#_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "#openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ce61227efb2672",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T21:52:05.114718Z",
     "start_time": "2024-10-06T21:52:03.799307Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loaders = [\n",
    "    # Duplicate documents on purpose - messy data\n",
    "    PyPDFLoader(\"pdf/book.pdf\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0f3461a1475565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T21:52:06.971430Z",
     "start_time": "2024-10-06T21:52:06.960322Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75195840fddd3be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T10:53:14.613053Z",
     "start_time": "2024-09-28T10:53:11.760352Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/ivy/anaconda3/lib/python3.11/site-packages/binwalk-2.3.3-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: langchain-openai in /Users/ivy/anaconda3/lib/python3.11/site-packages (0.2.1)\r\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-openai) (0.3.6)\r\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-openai) (1.50.1)\r\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-openai) (0.7.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (6.0.1)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (1.33)\r\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (0.1.128)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (23.2)\r\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (2.9.2)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (8.2.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (4.11.0)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.2.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.8.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.27.0)\r\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.5.0)\r\n",
      "Requirement already satisfied: sniffio in /Users/ivy/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.0)\r\n",
      "Requirement already satisfied: tqdm>4 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.65.0)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2023.10.3)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.31.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.4)\r\n",
      "Requirement already satisfied: certifi in /Users/ivy/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (2024.8.30)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ivy/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.0.5)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (0.14.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain-openai) (2.1)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-openai) (3.10.3)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-openai) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-openai) (2.23.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.7)\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1e492bbaa17e40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T10:53:19.047044Z",
     "start_time": "2024-09-28T10:53:18.999928Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use if  Not local embedding gonna be used \n",
    "embedding = OpenAIEmbeddings()\n",
    "persist_directory = 'docs/chroma/'\n",
    "!rm -rf ./docs/chroma  # remove old database files if any\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a37ae434f7c67b45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T21:52:13.820408Z",
     "start_time": "2024-10-06T21:52:12.487777Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If  local embedding gonna be used\n",
    "embedding = ollama.embeddings(\n",
    "  model='mxbai-embed-large',# also nomic-embed-text avilable\n",
    "  prompt='Llamas are members of the camelid family',\n",
    ")\n",
    "persist_directory = 'docs/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b195b8b12b5fa4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T21:52:24.167326Z",
     "start_time": "2024-10-06T21:52:24.034103Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf ./docs/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7143cc1ebf1018f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T21:52:42.841404Z",
     "start_time": "2024-10-06T21:52:42.746166Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e761063db38e7ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T21:53:28.473362Z",
     "start_time": "2024-10-06T21:52:47.324608Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#store each document in a vector embedding database\n",
    "for i, d in enumerate(splits):\n",
    "    page_content = str(d.page_content)    \n",
    "    try:\n",
    "        response = ollama.embeddings(\n",
    "            model=\"mxbai-embed-large\", \n",
    "            prompt=page_content\n",
    "        )\n",
    "        embedding = response[\"embedding\"]\n",
    "        collection.add(\n",
    "            ids=[str(i)],\n",
    "            embeddings=[embedding],\n",
    "            documents=[page_content], \n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9076c4e3a07b6ddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T22:03:44.936151Z",
     "start_time": "2024-10-06T22:03:05.798702Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "persist_directory = 'docs/chroma/'\n",
    "embedding_model = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "# Wrap the Chroma collection using langchain's Chroma vector store wrapper\n",
    "vectordb = Chroma.from_documents(\n",
    "     documents=splits,\n",
    "     embedding=embedding_model,\n",
    "     persist_directory=persist_directory\n",
    " )\n",
    "#vector_store.add_documents(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933a467b9ab54f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da43842b65fb485f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T22:06:51.332633Z",
     "start_time": "2024-10-06T22:06:51.287933Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### LLM\n",
    "local_llm = 'llama3.2:1b-instruct-fp16' #'llama3.2' \n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "### Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a16077f9a15cd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    llm_json_mode = ChatOllama(model=local_llm, temperature=0, format='json')\n",
    "    \n",
    "    router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "    \n",
    "    The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "                                        \n",
    "    Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n",
    "    \n",
    "    Return JSON with single key, datasource, that is 'websearch' or 'vectorstore' depending on the question.\"\"\"\n",
    "    \n",
    "    # Test router\n",
    "    test_web_search = llm_json_mode.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=\"Who is favored to win the NFC Championship game in the 2024 season?\")])\n",
    "    test_web_search_2 = llm_json_mode.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=\"What are the models released today for llama3.2?\")])\n",
    "    test_vector_store = llm_json_mode.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=\"What are the types of agent memory?\")])\n",
    "    print(json.loads(test_web_search.content), json.loads(test_web_search_2.content), json.loads(test_vector_store.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82621da54159746e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T22:06:51.989285Z",
     "start_time": "2024-10-06T22:06:51.982027Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create chain \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "287f6d7a77c1b9a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T22:07:04.378949Z",
     "start_time": "2024-10-06T22:06:52.746022Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A beachhead market is not explicitly defined in the provided text, but based on the context, I can make some educated guesses.\\n\\nIt appears to be related to a business strategy or marketing approach. Here are a few possible interpretations:\\n\\n1. **Beachhead market**: In military and strategic planning, a beachhead refers to a strategic location where an army or force establishes a foothold in a hostile territory. Similarly, in business, a beachhead market is a strategic area where a company establishes its presence and gains traction.\\n2. **Market entry strategy**: A beachhead market could refer to the initial market entry strategy for a new product, service, or business. It's a critical step where a company decides which markets to focus on first.\\n\\nThe specific steps mentioned in the text seem to be related to the following:\\n\\n* STEP 1: Selecting a Beachhead Market (41)\\n* STEP 2: Analyzing Top 6-12 Market Opportunities and Choosing One to Pursue\\n* STEP 3: Profiling the Persona for the Beachhead Market\\n* STEP 4: Identifying Adjacent Markets to Sell After Dominating the Beachhead Market\\n\\nBased on this, it seems that the beachhead market is a critical step in the business strategy process, where a company selects an initial market area to focus on and develops its product or service offerings for that market.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test 1\n",
    "question = \"what is beachead market? and which step is it in?\"\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "560586f01347a21d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T22:25:00.907612Z",
     "start_time": "2024-09-28T22:25:00.905022Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepate template\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d1326df0e6381a80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T22:25:03.756942Z",
     "start_time": "2024-09-28T22:25:00.907740Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thanks for asking! \\n\\nA selectable market refers to the first market that you can realistically and efficiently enter with your existing resources, without having to wait for a larger or more established market to mature. It\\'s the initial market where you can gain high exposure among potential customers and test your product or service before expanding into other markets.\\n\\nIn other words, it\\'s the \"first shot\" at success in a new market, rather than trying to enter a large or well-established market that may take time to develop.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test template\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "question = 'what is selectable market in step 1'\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e9ba3540b6966ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T22:25:03.757249Z",
     "start_time": "2024-09-28T22:25:03.754583Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'page': 58, 'source': 'pdf/book.pdf'}, page_content='STEP 2\\nSelect a Beachhead Market\\n41')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show where it is \n",
    "result[\"source_documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cb6a8adf73e0d86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T22:26:02.243661Z",
     "start_time": "2024-09-28T22:26:00.080371Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know, thanks for asking!\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test but with info. \n",
    "# Rag doesnt work here\n",
    "# template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "# {context}\n",
    "# Question: {question}\n",
    "# Helpful Answer:\"\"\"\n",
    "# question = 'what is beachead market?'\n",
    "# qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "#                                        retriever=vectordb.as_retriever(),\n",
    "#                                        return_source_documents=True,\n",
    "#                                        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "# \n",
    "# \n",
    "# result = qa_chain({\"query\": question})\n",
    "# result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2a4e746111597caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T22:26:13.732581Z",
     "start_time": "2024-09-28T22:26:10.279829Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A beachhead market refers to a position or state where a business has established itself with positive cash flow before it runs out, allowing for quick achievement of positive word of mouth (WOM) that can be a source of success or failure. In military operations, a beachhead strategy involves establishing a base of operations in enemy territory to launch further attacks and capture adjacent areas. For entrepreneurs, identifying a beachhead market is crucial as it enables them to establish a strong foundation for future growth and expansion.\n"
     ]
    }
   ],
   "source": [
    "### Generate this is good.\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "\n",
    "Here is the context to use to answer the question:\n",
    "\n",
    "{context} \n",
    "\n",
    "Think carefully about the above context. \n",
    "\n",
    "Now, review the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Provide an answer to this questions using only the above context. \n",
    "\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Test\n",
    "docs = vectordb.as_retriever().invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "print(generation.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "91f3b169a756dd53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T22:47:42.914077Z",
     "start_time": "2024-09-28T22:47:42.458815Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 00:47:42.907 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/ivy/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-09-29 00:47:42.908 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "# Set the title of the app\n",
    "st.title(\"Chatbot with Document Upload\")\n",
    "\n",
    "# Chatbot area\n",
    "st.header(\"Chat with AI\")\n",
    "user_input = st.text_input(\"Enter your message:\")\n",
    "chat_response = \"\"\n",
    "\n",
    "if user_input:\n",
    "    # Replace with your server/chatbot API URL\n",
    "    api_url = \"http://your_server_endpoint/chatbot\"\n",
    "    response = requests.post(api_url, json={\"message\": user_input})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        chat_response = response.json().get(\"response\", \"\")\n",
    "    else:\n",
    "        chat_response = \"Error: Couldn't fetch the response from the chatbot.\"\n",
    "\n",
    "    st.write(\"Chatbot:\", chat_response)\n",
    "\n",
    "# Divider\n",
    "st.markdown(\"---\")\n",
    "\n",
    "# Document Upload Area\n",
    "st.header(\"Upload Your Documents\")\n",
    "uploaded_file = st.file_uploader(\"Choose a file\", type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Display uploaded file name\n",
    "    st.write(\"File uploaded:\", uploaded_file.name)\n",
    "    \n",
    "    # Optional: If you want to send the file to the backend\n",
    "    files = {\"file\": uploaded_file.getvalue()}\n",
    "    \n",
    "    # Replace with your server endpoint that handles document processing\n",
    "    upload_url = \"http://your_server_endpoint/upload\"\n",
    "    response = requests.post(upload_url, files=files)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        st.write(\"Document uploaded successfully!\")\n",
    "    else:\n",
    "        st.write(\"Failed to upload the document.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8578045cc07e5f08",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
